- 퍼셉트론에서 신경망으로
    - 신경망의 예
        - 입력층 : 신경망의 가장 왼쪽 줄
        - 출력층 : 신경망의 맨 오른쪽 줄
        - 은닉층 : 그 사이 중간 줄
    - 퍼셉트론 복습
        - 편향은 뉴런이 얼마나 쉽게 활성화되느냐를 제어함
        - 가중치는 각 신호의 영향력을 제어함
    - 활성화 함수의 등장
        - 활성화 함수 : 입력신호의 총합을 출력신호로 변환하는 함수
            
            ⇒ 입력신호의 총합이 활성화를 일으키는지를 정하는 역할
            
        - w_1, w_2, b를 조합한 결과가 a가 된다면, 활성화 함수 y=h(a)가 노드의 결과값이 됨
- 활성화 함수
    
    → 퍼셉트론은 계단함수(임계값을 경계로 출력이 바뀜) 활용
    
    - 시그모이드 함수
        
        $$
        h(x)=\frac{1}{1+\exp(-x)}
        $$
        
        - 신경망에서 자주 이용하는 활성화 함수
    - 계단함수 구현하기
        - numpy의 astype(np.int) 활용하여 간단히 가능
- 추가 공부
    - 신경망에서 활성화 함수로 쓰이는 것 중 ReLU 함수도 있음
        
        ```python
        def ReLU(x):
        	return max(0,x)
        ```
        
        → 역전파 시 가중치를 업데이트하기 어려워지는 기울기 소실 문제를 해결함
        
        → 희소활성화를 통해 계산량을 줄이고 과적합도 해소한다
        
    - 출력층의 활성화 함수로는 항등함수(회귀)나 소프트맥스함수(분류)를 사용한다.