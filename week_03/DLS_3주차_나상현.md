- 퍼셉트론에서 신경망으로
    - 신경망의 예
        - 입력층 : 신경망의 가장 왼쪽 줄
        - 출력층 : 신경망의 맨 오른쪽 줄
        - 은닉층 : 그 사이 중간 줄
    - 퍼셉트론 복습
        - 편향은 뉴런이 얼마나 쉽게 활성화되느냐를 제어함
        - 가중치는 각 신호의 영향력을 제어함
    - 활성화 함수의 등장
        - 활성화 함수 : 입력신호의 총합을 출력신호로 변환하는 함수
            
            ⇒ 입력신호의 총합이 활성화를 일으키는지를 정하는 역할
            
        - w_1, w_2, b를 조합한 결과가 a가 된다면, 활성화 함수 y=h(a)가 노드의 결과값이 됨
- 활성화 함수
    
    → 퍼셉트론은 계단함수(임계값을 경계로 출력이 바뀜) 활용
    
    - 시그모이드 함수
        
        $$
        h(x)=\frac{1}{1+\exp(-x)}
        $$
        
        - 신경망에서 자주 이용하는 활성화 함수
    - 계단함수
        - numpy의 astype(np.int) 활용하여 간단히 구현 가능
        - 비선형 함수
            - 선형 함수를 사용하면 신경망을 깊게하는 의미가 없음.
- 다차원 배열의 계산
    - `np.ndim()` 함수를 통해 배열의 차원 수를 알 수 있음
    - `np.shape` 을 통해 배열의 형상을 알 수 있음
- 행렬의 곱
    - `np.dot()` 을 통해 행렬의 곱 연산 가능
        
        → 선형대수학에서와 동일하게, 두 행렬의 대응하는 차원의 원소 수 일치해야
        
    - 1행 2열의 행렬 x_1, x_2를 [[1,3,5],[2,4,6]]에 행렬곱하면 3행1열의 결과값 y_1, y_2, y_3이 나옴
- 출력층 설계하기
    - 항등함수 : 입력을 그대로 출력
    - 소프트맥스 함수
    
    $$
    y_k = \frac{\exp(a_k)}{\sum^n_{i=1}\exp(a_i)}
    $$
    
    → 근데 이렇게 쓰면 오버플로우 발생 가능, 그래서 수정
    
    $$
    \frac{\exp(a_k+C^\prime)}{\sum^n_{i=1}\exp(a_i+C^\prime)}
    $$
    
    ++ 소프트맥스 함수는 0에서 1 사이의 실수를 출력하기 때문에 확률로 해석할 수 있음
    
- 손글시 숫자 인식
    - MNIST 데이터셋
        - 이미지 데이터를 np의 ndarray로 변환하여 사용, Image.fromarray()로 다시 이미지로 변환할 수 있음
        - 배치 : 하나로 묶은 입력 데이터
            
            ⇒ 데이터를 배치로 처리하면 빠르고 효율적으로 처리할 수 있다
            
- 추가
    - 신경망에서 활성화 함수로 쓰이는 것 중 ReLU 함수
        
        ```python
        def ReLU(x):
        	return max(0,x)
        ```
        
        → 희소활성화를 통해 계산량을 줄이고 과적합도 해소한다
        
        → 역전파 시 가중치를 업데이트하기 어려워지는 기울기 소실 문제를 해결함
        
    - 출력층의 활성화 함수로는 항등함수(회귀)나 소프트맥스함수(분류)를 사용한다.